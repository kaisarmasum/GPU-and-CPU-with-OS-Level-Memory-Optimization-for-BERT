{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2785615e-20e4-4281-bb53-03c2e7d0d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abuka\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1260c728-9a69-486d-ac68-696e069fe896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 25.7M/25.7M [00:58<00:00, 460kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\abuka\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11867ff-acd3-4f8a-9c53-9b45404e9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abuka\\anaconda3\\envs\\torch\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abuka\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\abuka\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Validation Accuracy: 0.8851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88      4961\n",
      "           1       0.85      0.94      0.89      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.88      0.88     10000\n",
      "weighted avg       0.89      0.89      0.88     10000\n",
      "\n",
      "Epoch 2/4\n",
      "Validation Accuracy: 0.8934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89      4961\n",
      "           1       0.86      0.94      0.90      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n",
      "Epoch 3/4\n",
      "Validation Accuracy: 0.8970\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      4961\n",
      "           1       0.90      0.89      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Epoch 4/4\n",
      "Validation Accuracy: 0.8978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      4961\n",
      "           1       0.90      0.90      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_data(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    texts = df['review'].tolist()\n",
    "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
    "    return texts, labels\n",
    "\n",
    "data_file = \"/Users/abuka/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\"\n",
    "texts, labels = load_imdb_data(data_file)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return \"positive\" if preds.item() == 1 else \"negative\"\n",
    "\n",
    "\n",
    "# Set up parameters\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    accuracy, report = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "torch.save(model.state_dict(), \"bert_text_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67328fb-30e6-43a9-9f50-8cfa62c82c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was great and I really enjoyed the performances of the actors.\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Test sentiment prediction\n",
    "test_text = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(\"The movie was great and I really enjoyed the performances of the actors.\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2b66d5-1a8d-4734-8fc7-e85476a73f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f1cf6c-de11-435b-ae5e-ef0342159ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  model_inference        32.54%       5.468ms       100.00%      16.806ms      16.806ms             1  \n",
      "                                     aten::linear         5.94%     998.300us        41.24%       6.932ms      93.669us            74  \n",
      "                                      aten::addmm        25.18%       4.232ms        25.18%       4.232ms      57.184us            74  \n",
      "                                 aten::layer_norm         0.43%      71.600us         6.55%       1.102ms      44.064us            25  \n",
      "                          aten::native_layer_norm         4.81%     808.000us         6.13%       1.030ms      41.200us            25  \n",
      "               aten::scaled_dot_product_attention         0.85%     142.100us         5.82%     977.400us      81.450us            12  \n",
      "                                          aten::t         3.92%     659.100us         5.61%     943.000us      12.743us            74  \n",
      "    aten::_scaled_dot_product_efficient_attention         1.49%     249.800us         4.97%     835.300us      69.608us            12  \n",
      "                                       aten::view         4.33%     726.900us         4.33%     726.900us       2.931us           248  \n",
      "                                        aten::add         4.30%     722.800us         4.30%     722.800us      28.912us            25  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 16.806ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure your model is on the same device as the inputs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize the inputs and create the attention mask\n",
    "inputs = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "encoding = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Move the tokenized inputs to the same device as the model\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Explicitly pass only 'input_ids' and 'attention_mask' to the model\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b46f03-c553-4fcf-9d6e-0c14a54b5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  model_inference        28.74%      15.287ms       100.00%      53.192ms      53.192ms       4.069ms         5.33%      76.391ms      76.391ms             1  \n",
      "                                     aten::linear        19.40%      10.321ms        45.29%      24.092ms     325.562us       3.003ms         3.93%      59.797ms     808.068us            74  \n",
      "                                      aten::addmm        12.87%       6.843ms        12.87%       6.843ms      92.478us      49.764ms        65.14%      49.764ms     672.486us            74  \n",
      "               aten::scaled_dot_product_attention         1.16%     615.600us         8.68%       4.617ms     384.792us     102.000us         0.13%       4.966ms     413.833us            12  \n",
      "    aten::_scaled_dot_product_efficient_attention         2.65%       1.411ms         7.52%       4.002ms     333.492us     663.000us         0.87%       4.864ms     405.333us            12  \n",
      "                                 aten::layer_norm         1.51%     803.900us         9.54%       5.072ms     202.892us       1.427ms         1.87%       4.817ms     192.680us            25  \n",
      "                                          aten::t         3.86%       2.054ms         7.34%       3.902ms      52.732us       2.100ms         2.75%       4.739ms      64.041us            74  \n",
      "                                  aten::transpose         5.65%       3.008ms         5.98%       3.178ms      23.719us       2.410ms         3.15%       4.350ms      32.463us           134  \n",
      "                          aten::native_layer_norm         7.14%       3.799ms         8.02%       4.268ms     170.736us       2.545ms         3.33%       3.390ms     135.600us            25  \n",
      "               aten::_efficient_attention_forward         2.45%       1.304ms         2.83%       1.506ms     125.525us       2.040ms         2.67%       2.615ms     217.917us            12  \n",
      "                                    aten::reshape         4.41%       2.347ms         5.41%       2.879ms      33.087us       1.552ms         2.03%       2.347ms      26.977us            87  \n",
      "                                 aten::as_strided         0.44%     234.900us         0.44%     234.900us       1.320us       2.116ms         2.77%       2.116ms      11.888us           178  \n",
      "                                       aten::view         2.38%       1.268ms         2.38%       1.268ms       5.112us       1.555ms         2.04%       1.555ms       6.270us           248  \n",
      "                                      aten::empty         1.12%     594.400us         1.12%     594.400us       4.717us       1.092ms         1.43%       1.092ms       8.667us           126  \n",
      "                                        aten::add         2.18%       1.159ms         2.18%       1.159ms      46.376us     507.000us         0.66%     507.000us      20.280us            25  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 53.192ms\n",
      "Self CUDA time total: 76.391ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the inputs and create the attention mask\n",
    "inputs = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "encoding = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Move the tokenized inputs to the GPU (if available) to match the model's device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Set up profiling to track both CPU and GPU activities (GPU profiling requires CPU activities to be enabled)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        # Perform inference\n",
    "        model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Print out the GPU profiling results\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))  # Sort by GPU time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87cc5210-4c18-4ec0-8b66-3da519b2b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abuka\\AppData\\Local\\Temp\\ipykernel_8580\\2736761291.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert_text_classifier.pth\"))  # Load the saved model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.0524 seconds\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  model_inference        29.72%       9.996ms       100.00%      33.632ms      33.632ms       6.824ms        20.28%      33.642ms      33.642ms             1  \n",
      "                                     aten::linear        14.00%       4.708ms        40.14%      13.500ms     182.439us       2.502ms         7.44%      15.995ms     216.149us            74  \n",
      "                                      aten::addmm        12.59%       4.233ms        12.59%       4.233ms      57.203us       7.992ms        23.76%       7.992ms     108.000us            74  \n",
      "                                 aten::layer_norm         1.32%     443.000us        10.03%       3.373ms     134.932us     879.000us         2.61%       3.627ms     145.080us            25  \n",
      "                                  aten::transpose         8.19%       2.755ms         8.70%       2.926ms      21.836us       2.355ms         7.00%       3.384ms      25.254us           134  \n",
      "                                          aten::t         4.17%       1.402ms         9.11%       3.065ms      41.416us       1.390ms         4.13%       3.346ms      45.216us            74  \n",
      "               aten::scaled_dot_product_attention         0.75%     252.500us         9.98%       3.356ms     279.708us     217.000us         0.65%       3.299ms     274.917us            12  \n",
      "    aten::_scaled_dot_product_efficient_attention         3.03%       1.018ms         9.23%       3.104ms     258.667us     573.000us         1.70%       3.082ms     256.833us            12  \n",
      "                          aten::native_layer_norm         7.61%       2.559ms         8.71%       2.930ms     117.212us       1.634ms         4.86%       2.748ms     109.920us            25  \n",
      "                                    aten::reshape         3.89%       1.308ms         4.71%       1.584ms      18.209us       1.642ms         4.88%       2.358ms      27.103us            87  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 33.632ms\n",
      "Self CUDA time total: 33.642ms\n",
      "\n",
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import BertTokenizer\n",
    "import time\n",
    "\n",
    "# Ensure the model is on the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model = BERTClassifier(bert_model_name='bert-base-uncased', num_classes=2)  # Define the model structure as before\n",
    "model.load_state_dict(torch.load(\"bert_text_classifier.pth\"))  # Load the saved model weights\n",
    "model = model.to(device)  # Move the model to GPU if available\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the input text for inference\n",
    "inputs = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoding = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Move the tokenized inputs to the GPU (if available) to match the model's device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Start profiling GPU and inference time\n",
    "start_time = time.time()  # Start the overall inference time\n",
    "\n",
    "# Set up profiling to track both CPU and GPU activities (for inference time profiling)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        # Perform inference with the loaded model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# End of overall inference time\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "\n",
    "# Print out the GPU profiling results (to analyze GPU usage)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))  # Sort by GPU time\n",
    "\n",
    "# Get prediction result\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "prediction = \"positive\" if preds.item() == 1 else \"negative\"\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa285660-2fff-4a71-b4d2-da94fe4d05b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abuka\\AppData\\Local\\Temp\\ipykernel_8580\\4227628028.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert_text_classifier.pth\"))  # Load the saved model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.0430 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FunctionEventAvg' object has no attribute 'activity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Collect CPU and CUDA profiling data\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m prof\u001b[38;5;241m.\u001b[39mkey_averages():\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Extract CPU-related profiling data\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mactivity \u001b[38;5;241m==\u001b[39m ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU:\n\u001b[0;32m     54\u001b[0m         self_cpu_percent \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mself_cpu_time_total \u001b[38;5;241m/\u001b[39m key\u001b[38;5;241m.\u001b[39mcpu_time_total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mcpu_time_total \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     55\u001b[0m         profiling_data\u001b[38;5;241m.\u001b[39mappend([\n\u001b[0;32m     56\u001b[0m             key\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mself_cpu_percent\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m             key\u001b[38;5;241m.\u001b[39mcount\n\u001b[0;32m     67\u001b[0m         ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FunctionEventAvg' object has no attribute 'activity'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import BertTokenizer\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Ensure the model is on the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model = BERTClassifier(bert_model_name='bert-base-uncased', num_classes=2)  # Define the model structure as before\n",
    "model.load_state_dict(torch.load(\"bert_text_classifier.pth\"))  # Load the saved model weights\n",
    "model = model.to(device)  # Move the model to GPU if available\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the input text for inference\n",
    "inputs = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoding = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Move the tokenized inputs to the GPU (if available) to match the model's device\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Start profiling GPU and inference time\n",
    "start_time = time.time()  # Start the overall inference time\n",
    "\n",
    "# Set up profiling to track both CPU and GPU activities (for inference time profiling)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        # Perform inference with the loaded model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# End of overall inference time\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "\n",
    "# Prepare profiling data for saving in a CSV\n",
    "csv_file = 'profiling_results.csv'\n",
    "header = [\n",
    "    'Name', \n",
    "    'Self CPU %', 'Self CPU', 'CPU total %', 'CPU total', 'CPU time avg', \n",
    "    'Self CUDA', 'Self CUDA %', 'CUDA total', 'CUDA time avg', '# of Calls'\n",
    "]\n",
    "profiling_data = []\n",
    "\n",
    "# Collect CPU and CUDA profiling data\n",
    "for key in prof.key_averages():\n",
    "    # Extract CPU-related profiling data\n",
    "    if key.activity == ProfilerActivity.CPU:\n",
    "        self_cpu_percent = key.self_cpu_time_total / key.cpu_time_total * 100 if key.cpu_time_total > 0 else 0\n",
    "        profiling_data.append([\n",
    "            key.key,\n",
    "            f'{self_cpu_percent:.2f}%', \n",
    "            f'{key.self_cpu_time_total / 1000:.4f}',  # Convert to milliseconds\n",
    "            f'{key.cpu_time_total / 1000:.4f}',  # Convert to milliseconds\n",
    "            f'{key.cpu_time_avg / 1000:.4f}',  # Convert to milliseconds\n",
    "            f'{key.self_cpu_time_avg / 1000:.4f}',  # Convert to milliseconds\n",
    "            '0.00',  # Placeholder for CUDA data\n",
    "            '0.00%',  # Placeholder for CUDA percentage\n",
    "            '0.00',  # Placeholder for CUDA total time\n",
    "            '0.00',  # Placeholder for CUDA avg time\n",
    "            key.count\n",
    "        ])\n",
    "    # Extract CUDA-related profiling data\n",
    "    elif key.activity == ProfilerActivity.CUDA:\n",
    "        self_cuda_percent = key.self_cuda_time_total / key.cuda_time_total * 100 if key.cuda_time_total > 0 else 0\n",
    "        profiling_data.append([\n",
    "            key.key,\n",
    "            '0.00%',  # Placeholder for CPU data\n",
    "            '0.00',  # Placeholder for CPU time\n",
    "            '0.00',  # Placeholder for CPU total time\n",
    "            '0.00',  # Placeholder for CPU avg time\n",
    "            '0.00',  # Placeholder for CPU self time avg\n",
    "            f'{key.self_cuda_time_total / 1000:.4f}',  # Convert to milliseconds\n",
    "            f'{self_cuda_percent:.2f}%',  # Percentage for CUDA\n",
    "            f'{key.cuda_time_total / 1000:.4f}',  # Convert to milliseconds\n",
    "            f'{key.cuda_time_avg / 1000:.4f}',  # Convert to milliseconds\n",
    "            key.count\n",
    "        ])\n",
    "\n",
    "# Write the profiling data to a CSV file\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)  # Write header row\n",
    "    writer.writerows(profiling_data)  # Write the collected profiling data\n",
    "\n",
    "print(f\"Profiling results saved to {csv_file}\")\n",
    "\n",
    "# Get prediction result\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "prediction = \"positive\" if preds.item() == 1 else \"negative\"\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93ebd1-bd51-45f1-9026-03ec663b36c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
